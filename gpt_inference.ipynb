{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End2end_SP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o-mini\"\n",
    "data_file = \"/home/seonjeongh/DCAQG/box/data/ReCo.dcqg.test.json\"\n",
    "batch_nickname = \"end2end_CoT_v2\"\n",
    "\n",
    "OPEN_KEY=open(\"/home/seonjeongh/DCAQG/api_key.txt\").read()\n",
    "\n",
    "import json, jsonlines, re, sys, tqdm, os, nltk, time\n",
    "from collections import Counter\n",
    "import evaluate\n",
    "from openai import OpenAI\n",
    "#client = OpenAI(api_key=OPEN_KEY)\n",
    "\n",
    "prompt_template = open(f\"prompts/{batch_nickname}.txt\").read()\n",
    "os.makedirs(f\"outputs/{batch_nickname}\", exist_ok=True)\n",
    "\n",
    "def save_input_batch_file(input_batch_name, batch_inputs):\n",
    "    with open(input_batch_name, \"w\", encoding=\"UTF-8\") as fout:\n",
    "        for line in batch_inputs:\n",
    "            json.dump(line, fout, ensure_ascii=False)\n",
    "            fout.write(\"\\n\")\n",
    "        \n",
    "def run_batch(input_batch_name, batch_nickname):\n",
    "    print(\"### Run Batch\")\n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(input_batch_name, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "    batch = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": batch_nickname\n",
    "        }\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        batch_current = client.batches.retrieve(batch.id)\n",
    "        print(batch_current.status)\n",
    "        if batch_current.status == \"completed\":\n",
    "            output_file_id = batch_current.output_file_id\n",
    "            break\n",
    "        elif batch_current.status == \"failed\":\n",
    "            assert False, \"FAIL\"\n",
    "        time.sleep(30)\n",
    "        \n",
    "    return output_file_id\n",
    "\n",
    "def save_result(output_file_id, batch_nickname):\n",
    "    print(\"### Save Result\")\n",
    "    file_response = client.files.content(output_file_id)\n",
    "    with open(f\"outputs/{batch_nickname}/{batch_nickname}.gptBatchOutput.jsonl\", \"w\") as fout:\n",
    "        for line in file_response.text.split(\"\\n\"):\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            fout.write(line+\"\\n\")\n",
    "    return file_response\n",
    "            \n",
    "def calc_price(line):\n",
    "    model_name = line[\"response\"][\"body\"][\"model\"]\n",
    "    input_length = line[\"response\"][\"body\"][\"usage\"][\"prompt_tokens\"]\n",
    "    output_length = line[\"response\"][\"body\"][\"usage\"][\"completion_tokens\"]\n",
    "    \n",
    "    if model_name == \"gpt-4o-2024-08-06\":\n",
    "        return 0.00000125 * input_length + 0.000005 * output_length\n",
    "    elif model_name == \"gpt-4o-mini-2024-07-18\":\n",
    "        return 0.000000075 * input_length + 0.0000003 * output_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### START!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 22526.92it/s]\n"
     ]
    }
   ],
   "source": [
    "data_list = json.load(open(data_file))\n",
    "batch_inputs = []\n",
    "for data in tqdm.tqdm(data_list):\n",
    "    id = data[\"id\"]\n",
    "    \n",
    "    if data[\"reasoning_complexity\"] == \"NEI\":\n",
    "        reasoning_type = \"Not Enough Information\"\n",
    "        evidence_scope = \"Insufficient\"\n",
    "    else:\n",
    "        reasoning_type = data[\"reasoning_complexity\"].split(\"_\")[1]\n",
    "        evidence_scope = \"Single\" if \"single\" in data[\"reasoning_complexity\"] else \"Inter\"\n",
    "    \n",
    "    prompt = prompt_template.replace(\"{ document }\", data[\"document\"])\n",
    "    prompt = prompt.replace(\"{ passage_length }\", data[\"passage_length\"])\n",
    "    prompt = prompt.replace(\"{ sentence_length }\", data[\"sentence_length\"])\n",
    "    prompt = prompt.replace(\"{ vocab_level }\", data[\"vocab_level\"])\n",
    "    prompt = prompt.replace(\"{ statement_propositions }\", str(data[\"statement_propositions\"]))\n",
    "    prompt = prompt.replace(\"{ reasoning_type }\", reasoning_type)\n",
    "    prompt = prompt.replace(\"{ evidence_scope }\", evidence_scope)\n",
    "    \n",
    "    line = {\"custom_id\": id, \n",
    "            \"method\": \"POST\", \n",
    "            \"url\": \"/v1/chat/completions\", \n",
    "            \"body\": {\"model\": model_name, \n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}]}\n",
    "            }   \n",
    "    batch_inputs.append(line)\n",
    "\n",
    "input_batch_name = f\"outputs/{batch_nickname}/{batch_nickname}.gptBatchInput.jsonl\"\n",
    "save_input_batch_file(input_batch_name, batch_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Run Batch\n",
      "validating\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "in_progress\n",
      "finalizing\n",
      "completed\n",
      "### Save Result\n"
     ]
    }
   ],
   "source": [
    "output_file_id = run_batch(input_batch_name, batch_nickname)\n",
    "file_response = save_result(output_file_id, batch_nickname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Read Result\n",
      "outputs/end2end_CoT_v2/end2end_CoT_v2.gptBatchOutput.jsonl\n",
      "cost: 0.19355684999999986\n"
     ]
    }
   ],
   "source": [
    "### Read Result\n",
    "print(\"### Read Result\")\n",
    "\n",
    "def extract_passage_and_statement(text):\n",
    "    match = re.search(r'\\{\\s*\"passage\"\\s*:\\s*\".+?\",\\s*\"statement\"\\s*:\\s*\".+?\"\\s*\\}', text, re.DOTALL)\n",
    "\n",
    "    if match:\n",
    "        json_str = match.group(0)\n",
    "        try:\n",
    "            result = json.loads(json_str)\n",
    "            return result\n",
    "        except json.JSONDecodeError:\n",
    "            forward, backward = json_str.split('\"statement\": ')\n",
    "            passage = forward[len('{\\n  \\\"passage\\\": \\\"'):-len('\\\",\\n  ')]\n",
    "            statement = backward[1:-len('\\\"\\n}')]\n",
    "            return {\"passage\": passage, \"statement\": statement}\n",
    "    else:\n",
    "        print(\"No match\")\n",
    "        return None\n",
    "\n",
    "cost = 0\n",
    "id2prediction = dict()\n",
    "output_batch_file = f\"outputs/{batch_nickname}/{batch_nickname}.gptBatchOutput.jsonl\"\n",
    "print(output_batch_file)\n",
    "with jsonlines.open(output_batch_file) as f:\n",
    "    for input_prompt, line in zip(batch_inputs, f.iter()):\n",
    "        custom_id = line[\"custom_id\"]\n",
    "        response = line[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "        output = extract_passage_and_statement(response)\n",
    "        if output == None:\n",
    "            print(response)\n",
    "            assert False\n",
    "        \n",
    "        id2prediction[custom_id] = output\n",
    "        cost += calc_price(line)\n",
    "        \n",
    "print(\"cost:\", cost)\n",
    "with open(f\"outputs/{batch_nickname}/predictions.json\", \"w\") as fout:\n",
    "    json.dump(id2prediction, fout, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c0fc33fd294447b9b392d8e4f4dc07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498/498 [00:00<00:00, 1111635.65it/s]\n",
      "100%|██████████| 63/63 [02:21<00:00,  2.25s/it]\n",
      "100%|██████████| 498/498 [00:00<00:00, 1705112.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/seonjeongh/DCAQG/box/dcqg/difficulty_eval\")\n",
    "from evaluation import Evaluator\n",
    "\n",
    "evaluator = Evaluator(\"3,4\")\n",
    "\n",
    "id2prediction = json.load(open(f\"outputs/{batch_nickname}/predictions.json\"))\n",
    "\n",
    "\n",
    "passages, statements = [], []\n",
    "for id, pred_dict in tqdm.tqdm(id2prediction.items(), total=len(id2prediction.keys())):\n",
    "    passages.append(pred_dict[\"passage\"])\n",
    "    statements.append(pred_dict[\"statement\"])\n",
    "    \n",
    "predictions = evaluator.get_values(passages, statements)\n",
    "\n",
    "id2cal_prediction = dict()\n",
    "for id, prediction in tqdm.tqdm(zip(id2prediction.keys(), predictions), total=len(id2prediction.keys())):\n",
    "    id2cal_prediction[id] = prediction\n",
    "    \n",
    "with open(f\"outputs/{batch_nickname}/prediction_values.json\", \"w\") as fout:\n",
    "    json.dump(id2cal_prediction, fout, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "   \"passage_length\": 54.22,\n",
      "   \"sentence_length\": 74.1,\n",
      "   \"vocab_level\": 74.9,\n",
      "   \"statement_propositions\": 30.32,\n",
      "   \"reasoning_type\": null,\n",
      "   \"evidence_scope\": null\n",
      "}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/seonjeongh/DCAQG/box/dcqg/difficulty_eval\")\n",
    "from evaluation import eval\n",
    "\n",
    "id2cal_prediction = json.load(open(f\"outputs/{batch_nickname}/prediction_values.json\"))\n",
    "\n",
    "scores = eval(data_file, id2cal_prediction)\n",
    "with open(f\"outputs/{batch_nickname}/performances.json\", \"w\") as fout:\n",
    "    json.dump(scores, fout, indent=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hug4.29",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
